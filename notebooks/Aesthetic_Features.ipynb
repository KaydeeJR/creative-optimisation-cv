{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aesthetic Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By Aesthetic feature extraction we refer to the aesthetical components of an image this mainly refers to Color, Composition and Texture of an adcreative. This features are extracted on the top of the assumption the peoples are attracted to beauty as a whole eventhough different persons have different perception of beauty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageColor():\n",
    "    def __init__(self, image_path):\n",
    "        super(ImageColor, self).__init__()\n",
    "\n",
    "        # Read in the image\n",
    "        self.bgr_img = None\n",
    "        self.hsv_img = None\n",
    "        self.gray_img = None\n",
    "\n",
    "        self.h_mean, self.s_mean, self.v_mean = None, None, None\n",
    "        self.h_std, self.s_std, self.v_std = None, None, None\n",
    "\n",
    "        # Read in the image\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def update(self, image_path):\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def read_in(self, image_path):\n",
    "        # This function reads the image into the bgr(blue, green, red), hsv(hue, saturation, value) and gray_scale image.\n",
    "\n",
    "        self.bgr_img = cv2.imread(image_path)\n",
    "        self.hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "        self.gray_img = cv2.imread(image_path, 0)\n",
    "\n",
    "        # Compute the statics\n",
    "        self.h_mean, self.s_mean, self.v_mean = np.mean(self.hsv_img, axis=(0, 1))\n",
    "        self.h_std, self.s_std, self.v_std = np.std(self.hsv_img, axis=(0, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_circular(channel_image):\n",
    "        A = np.cos(channel_image).sum()\n",
    "        B = np.sin(channel_image).sum()\n",
    "\n",
    "        R = 1 - np.sqrt(A ** 2 + B ** 2) / (channel_image.shape[0] * channel_image.shape[1])\n",
    "\n",
    "        return R\n",
    "\n",
    "    def compute_hsv_statics(self):\n",
    "        h_circular = self.compute_circular(self.hsv_img[0])\n",
    "        v_intensity = np.sqrt((self.hsv_img[-1] ** 2).mean())\n",
    "\n",
    "        return [self.s_mean, self.s_std, self.v_std, h_circular, v_intensity]\n",
    "\n",
    "    def compute_emotion_based(self):\n",
    "        valence = 0.69 * self.v_mean + 0.22 * self.s_mean\n",
    "        arousal = -0.31 * self.v_mean + 0.6 * self.s_mean\n",
    "        dominance = -0.76 * self.v_mean + 0.32 * self.s_mean\n",
    "\n",
    "        return [valence, arousal, dominance]\n",
    "\n",
    "    def compute_valence(self):\n",
    "        valence = 0.69 * self.v_mean + 0.22 * self.s_mean\n",
    "\n",
    "        return valence\n",
    "\n",
    "    def compute_arousal(self):\n",
    "        arousal = -0.31 * self.v_mean + 0.6 * self.s_mean\n",
    "\n",
    "        return arousal\n",
    "\n",
    "    def compute_dominance(self):\n",
    "        dominance = -0.76 * self.v_mean + 0.32 * self.s_mean\n",
    "\n",
    "        return dominance\n",
    "\n",
    "    def compute_color_diversity(self):\n",
    "        \"\"\"Adapted from\n",
    "        https://github.com/yilangpeng/computational-aesthetics/blob/27ff52b47b880bd46a14a7b062a4dde69b6a9988/basic.py#L46-L56\n",
    "        \"\"\"\n",
    "        rgb = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2RGB).astype(float)\n",
    "\n",
    "        l_rgbR, l_rgbG, l_rgbB = cv2.split(rgb)\n",
    "        l_rg = l_rgbR - l_rgbG\n",
    "        l_yb = 0.5 * l_rgbR + 0.5 * l_rgbG - l_rgbB\n",
    "\n",
    "        rg_sd = np.std(l_rg)\n",
    "        rg_mean = np.mean(l_rg)\n",
    "        yb_sd = np.std(l_yb)\n",
    "        yb_mean = np.mean(l_yb)\n",
    "\n",
    "        rg_yb_sd = (rg_sd ** 2 + yb_sd ** 2) ** 0.5\n",
    "        rg_yb_mean = (rg_mean ** 2 + yb_mean ** 2) ** 0.5\n",
    "        colorful = rg_yb_sd + (rg_yb_mean * 0.3)\n",
    "\n",
    "        return [colorful]\n",
    "\n",
    "    def compute_color_info(self):\n",
    "        hsv_res = self.compute_hsv_statics()\n",
    "        emotion_res = self.compute_emotion_based()\n",
    "        color_div_res = self.compute_color_diversity()\n",
    "\n",
    "        return hsv_res + emotion_res + color_div_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.311312694085938]\n",
      "[137.71552599999998, -34.069985111111116, -129.31574844444444]\n"
     ]
    }
   ],
   "source": [
    "image = ImageColor(\"/home/michael_getachew/creative-optimization/creative-optimisation-cv/data/Challenge_Data/Assets/0a59be2e7dd53d6de11a10ce3649c081/_preview.png\")\n",
    "res_diversity = image.compute_color_diversity()\n",
    "res_emotion = image.compute_emotion_based()\n",
    "res_hsvstat = image.compute_hsv_statics()\n",
    "print(res_diversity)\n",
    "print(res_emotion)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.cluster import (\n",
    "    MeanShift,\n",
    "    estimate_bandwidth\n",
    ")\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Composition():\n",
    "    def __init__(self, image_path):\n",
    "        super(Composition, self).__init__()\n",
    "        self.bgr_img = None\n",
    "        self.hsv_img = None\n",
    "        self.gray_img = None\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def compute_edge_pixels(self,\n",
    "                            blur_size: int = 3,\n",
    "                            ratio_low=0.4, ratio_up=0.8):\n",
    "        \"\"\"Adapted from\n",
    "        https://github.com/yilangpeng/computational-aesthetics/blob/master/edge.py\n",
    "        \"\"\"\n",
    "        h, w = self.bgr_img.shape[:2]\n",
    "        blur_img = cv2.GaussianBlur(self.gray_img, (blur_size, blur_size), 0)\n",
    "\n",
    "        thresh_low = min(100, np.quantile(blur_img, q=ratio_low))\n",
    "        thresh_up = max(200, np.quantile(blur_img, q=ratio_up))\n",
    "\n",
    "        edges_img = cv2.Canny(blur_img,\n",
    "                              threshold1=thresh_low,\n",
    "                              threshold2=thresh_up)\n",
    "        num_edges = np.count_nonzero(edges_img) / (h * w)\n",
    "\n",
    "        return [num_edges]\n",
    "\n",
    "    def update(self, image_path):\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def read_in(self, image_path):\n",
    "        self.bgr_img = cv2.imread(image_path)\n",
    "        self.hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "        self.gray_img = cv2.imread(image_path, 0)\n",
    "\n",
    "    def compute_level_of_details(self,\n",
    "                                 quantile=0.2,\n",
    "                                 n_samples=3000,\n",
    "                                 thresh=0.05):\n",
    "        # Using quick shift segmentation\n",
    "        rgb_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Flatten the image\n",
    "        flat_image = rgb_img.reshape(-1, 3)\n",
    "        flat_image = np.float32(flat_image)\n",
    "\n",
    "        bandwidth = estimate_bandwidth(flat_image,\n",
    "                                       quantile=quantile,\n",
    "                                       n_samples=n_samples)\n",
    "        mean_shift = MeanShift(bandwidth, bin_seeding=True)\n",
    "        mean_shift.fit(flat_image)\n",
    "        image_labels = mean_shift.labels_\n",
    "\n",
    "        h, w = rgb_img.shape[:2]\n",
    "        unique_labels, unique_counts = np.unique(image_labels,\n",
    "                                                 return_counts=True)\n",
    "\n",
    "        # Remove small region of image\n",
    "        mean_thresh = (h * w) * thresh\n",
    "        unique_labels = unique_labels[unique_counts > mean_thresh]\n",
    "        unique_counts = unique_counts[unique_counts > mean_thresh]\n",
    "\n",
    "        num_seg = len(unique_labels)\n",
    "        average_size = unique_counts.mean() / (h * w)\n",
    "\n",
    "        return [num_seg, average_size]\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_channel_depth_of_field(channel):\n",
    "        level_wanted = channel[1]\n",
    "        h, w = level_wanted[0].shape[:2]\n",
    "\n",
    "        # Create blank image to include all channel\n",
    "        blank_image = np.zeros((h, w, 3))\n",
    "\n",
    "        for idx, level_wanted_matrix in enumerate(level_wanted):\n",
    "            blank_image[..., idx] = np.abs(level_wanted_matrix)\n",
    "\n",
    "        \n",
    "        # Compute M6, M7, M10, M11\n",
    "        start_x, end_x = int(h / 4), int(3 * h / 4)\n",
    "        start_y, end_y = int(w / 4), int(3 * w / 4)\n",
    "\n",
    "        dof_channel = np.sum(blank_image[start_x:end_x, start_y:end_y, :]) / np.sum(blank_image)\n",
    "\n",
    "        return dof_channel\n",
    "\n",
    "    def compute_depth_of_field(self):\n",
    "        # Process for the H channel\n",
    "        h_wavelet = pywt.wavedec2(self.hsv_img[..., 0], mode=\"periodization\",\n",
    "                                  wavelet=\"db3\", level=3)\n",
    "        h_dof = self.compute_channel_depth_of_field(h_wavelet)\n",
    "\n",
    "        # Process for the S channel\n",
    "        s_wavelet = pywt.wavedec2(self.hsv_img[..., 1], mode=\"periodization\",\n",
    "                                  wavelet=\"db3\", level=3)\n",
    "        s_dof = self.compute_channel_depth_of_field(s_wavelet)\n",
    "\n",
    "        # Process for the V channel\n",
    "        v_wavelet = pywt.wavedec2(self.hsv_img[..., 2], mode=\"periodization\",\n",
    "                                  wavelet=\"db3\", level=3)\n",
    "        v_dof = self.compute_channel_depth_of_field(v_wavelet)\n",
    "\n",
    "        return [h_dof, s_dof, v_dof]\n",
    "\n",
    "    def compute_h_dof(self):\n",
    "        # Process for the H channel\n",
    "        h_wavelet = pywt.wavedec2(self.hsv_img[..., 0], mode=\"periodization\",\n",
    "                                  wavelet=\"db3\", level=3)\n",
    "        h_dof = self.compute_channel_depth_of_field(h_wavelet)\n",
    "\n",
    "        return h_dof\n",
    "\n",
    "    def compute_s_dof(self):\n",
    "        # Process for the S channel\n",
    "        s_wavelet = pywt.wavedec2(self.hsv_img[..., 1], mode=\"periodization\",\n",
    "                                  wavelet=\"db3\", level=3)\n",
    "        s_dof = self.compute_channel_depth_of_field(s_wavelet)\n",
    "\n",
    "        return s_dof\n",
    "\n",
    "    def compute_v_dof(self):\n",
    "        # Process for the V channel\n",
    "        v_wavelet = pywt.wavedec2(self.hsv_img[..., 2], mode=\"periodization\",\n",
    "                                  wavelet=\"db3\", level=3)\n",
    "        v_dof = self.compute_channel_depth_of_field(v_wavelet)\n",
    "\n",
    "        return v_dof\n",
    "\n",
    "    def compute_rule_of_third(self):\n",
    "        h, w = self.bgr_img.shape[:2]\n",
    "\n",
    "        # Convert to hsv\n",
    "        hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Set the end points of image\n",
    "        start_h, end_h = int(h / 3), int(2 * h / 3)\n",
    "        start_w, end_w = int(w / 3), int(2 * w / 3)\n",
    "        center_image = hsv_img[start_h:end_h, start_w:end_w]\n",
    "\n",
    "        # Compute the mean of saturation and value\n",
    "        s_mean = np.mean(center_image[..., 1])\n",
    "        v_mean = np.mean(center_image[..., 2])\n",
    "\n",
    "        return [s_mean, v_mean]\n",
    "\n",
    "    def compute_s_mean(self):\n",
    "        h, w = self.bgr_img.shape[:2]\n",
    "\n",
    "        # Convert to hsv\n",
    "        hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Set the end points of image\n",
    "        start_h, end_h = int(h / 3), int(2 * h / 3)\n",
    "        start_w, end_w = int(w / 3), int(2 * w / 3)\n",
    "        center_image = hsv_img[start_h:end_h, start_w:end_w]\n",
    "\n",
    "        # Compute the mean of saturation and value\n",
    "        s_mean = np.mean(center_image[..., 1])\n",
    "\n",
    "        return s_mean\n",
    "\n",
    "    def compute_v_mean(self):\n",
    "        h, w = self.bgr_img.shape[:2]\n",
    "\n",
    "        # Convert to hsv\n",
    "        hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Set the end points of image\n",
    "        start_h, end_h = int(h / 3), int(2 * h / 3)\n",
    "        start_w, end_w = int(w / 3), int(2 * w / 3)\n",
    "        center_image = hsv_img[start_h:end_h, start_w:end_w]\n",
    "\n",
    "        # Compute the mean of saturation and value\n",
    "        v_mean = np.mean(center_image[..., 2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24078261539783, 0.36778763751914645, 0.33361000005577574]\n",
      "[0.04909]\n",
      "[0.24078261539783, 0.36778763751914645, 0.33361000005577574]\n",
      "[71.72994011976049, 75.76410179640719]\n"
     ]
    }
   ],
   "source": [
    "composition = Composition(\"/home/michael_getachew/creative-optimization/creative-optimisation-cv/data/Challenge_Data/Assets/0a22f881b77f00220f2034c21a18b854/_preview.png\")\n",
    "count_edges = composition.compute_depth_of_field()\n",
    "edge_res = composition.compute_edge_pixels()\n",
    "rule_of_thirds_res = composition.compute_rule_of_third()\n",
    "print(count_edges)\n",
    "print(edge_res)\n",
    "print(rule_of_thirds_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import skimage.measure\n",
    "import skimage.feature\n",
    "import skimage.morphology\n",
    "import skimage.filters.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Texture():\n",
    "    def __init__(self, image_path):\n",
    "        super(Texture, self).__init__()\n",
    "        self.bgr_img = None\n",
    "        self.hsv_img = None\n",
    "        self.gray_img = None\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def compute_entropy(self):\n",
    "        entropy = skimage.filters.rank.entropy(image=self.gray_img,\n",
    "                                               selem=skimage.morphology.square(9))\n",
    "        entropy = entropy.mean()\n",
    "        return [entropy]\n",
    "\n",
    "    def update(self, image_path):\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def read_in(self, image_path):\n",
    "        self.bgr_img = cv2.imread(image_path)\n",
    "        self.hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "        self.gray_img = cv2.imread(image_path, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_channel_wavlet(wavelets):\n",
    "        result_value = []\n",
    "        level_mean_value = 0.0\n",
    "        for wavelet in wavelets[1:]:\n",
    "            wavelet_feature = 0.0\n",
    "            magnitude = 0.0\n",
    "\n",
    "            temp_level_mean_value = 0\n",
    "            for level_list in wavelet:\n",
    "                wavelet_feature += np.sum(level_list)\n",
    "                magnitude += (level_list.shape[0] * level_list.shape[1])\n",
    "                temp_level_mean_value += np.mean(level_list)\n",
    "\n",
    "            level_mean_value += (temp_level_mean_value / 3)\n",
    "            result_value.append(wavelet_feature / magnitude)\n",
    "\n",
    "        result_value.append(level_mean_value)\n",
    "        return \n",
    "\n",
    "    \n",
    "    def coarseness(self, max_k=5):\n",
    "        \"\"\"Adapted from\n",
    "        https://github.com/Sdhir/TamuraFeatures/blob/f22f99a5898b2414e1c3f89d464a3a761e8b6a98/Tamura.m#L26-L191\n",
    "        https://github.com/MarshalLeeeeee/Tamura-In-Python/blob/1a079acce55989d65fb1a676e0bf6b9fbe54be82/tamura-numpy.py#L4-L37\n",
    "        \"\"\"\n",
    "        gray_img = np.copy(self.gray_img)\n",
    "        h, w = gray_img.shape\n",
    "        if 2 ** max_k >= w or 2 ** max_k >= h:\n",
    "            max_k = min(int(math.log(h) / math.log(2)), int(math.log(w) / math.log(2)))\n",
    "\n",
    "        average_gray = np.zeros((max_k, h, w))\n",
    "        for k in range(1, max_k + 1):\n",
    "            for row in range(2 ** (k-1), h - 2**(k-1)):\n",
    "                for col in range(2 ** (k-1), w - 2**(k-1)):\n",
    "                    if len(gray_img[row - 2**(k-1):row + 2**(k-1), col - 2**(k-1):col + 2**(k-1)]) == 0:\n",
    "                        assert 1 == 2\n",
    "                    average_gray[k - 1, row, col] = \\\n",
    "                        gray_img[row - 2**(k-1):row + 2**(k-1) + 1, col - 2**(k-1):col + 2**(k-1) + 1].mean()\n",
    "\n",
    "        expected_horizontal = np.zeros((max_k, h, w))\n",
    "        expected_vertical = np.zeros((max_k, h, w))\n",
    "\n",
    "        for k in range(1, max_k + 1):\n",
    "            for row in range(2**(k-1), h - 2**(k-1)):\n",
    "                for col in range(2**(k-1), w - 2**(k-1)):\n",
    "                    expected_horizontal[k - 1, row, col] = \\\n",
    "                        np.abs(\n",
    "                            average_gray[k - 1, row + 2**(k-1), col] - average_gray[k - 1, row - 2**(k-1), col])\n",
    "                    expected_vertical[k - 1, row, col] = \\\n",
    "                        np.abs(\n",
    "                            average_gray[k - 1, row, col + 2**(k-1)] - average_gray[k - 1, row, col - 2**(k-1)])\n",
    "\n",
    "        coarseness_best = np.zeros((h, w))\n",
    "        for row in range(h):\n",
    "            for col in range(w):\n",
    "                max_horizontal = np.max(expected_horizontal[:, row, col])\n",
    "                argmax_horizontal = np.argmax(expected_horizontal[:, row, col])\n",
    "                max_vertical = np.max(expected_vertical[:, row, col])\n",
    "                argmax_vertical = np.argmax(expected_vertical[:, row, col])\n",
    "\n",
    "                if max_horizontal > max_vertical:\n",
    "                    max_arg_k = argmax_horizontal\n",
    "                else:\n",
    "                    max_arg_k = argmax_vertical\n",
    "\n",
    "                coarseness_best[row, col] = 2 ** max_arg_k\n",
    "\n",
    "        coarseness = coarseness_best.mean()\n",
    "\n",
    "        return coarseness\n",
    "\n",
    "    def contrast(self):\n",
    "        \"\"\"Adapted from\n",
    "        https://github.com/Sdhir/TamuraFeatures/blob/f22f99a5898b2414e1c3f89d464a3a761e8b6a98/Tamura.m#L194-L206\n",
    "        \"\"\"\n",
    "        gray_img = np.copy(self.gray_img).reshape(-1)\n",
    "        average_value = np.mean(gray_img)\n",
    "\n",
    "        base_value = gray_img - average_value\n",
    "        fourth_moment = np.mean(np.power(base_value, 4))\n",
    "        variance = np.mean(np.power(base_value, 2))\n",
    "\n",
    "        alpha = fourth_moment / (variance ** 2)\n",
    "        contrast_value = math.sqrt(variance) / math.pow(alpha, 0.25)\n",
    "\n",
    "        return contrast_value\n",
    "\n",
    "    def directionality(self):\n",
    "        \"\"\"Adapted from\n",
    "        https://github.com/Sdhir/TamuraFeatures/blob/f22f99a5898b2414e1c3f89d464a3a761e8b6a98/Tamura.m#L209-L302\n",
    "        \"\"\"\n",
    "        # Padding image for the filter\n",
    "        gray_img = np.copy(self.gray_img).astype(\"int64\")\n",
    "        h, w = gray_img.shape[:2]\n",
    "\n",
    "        horizontal_filter = np.array([[-1, 0, 1],\n",
    "                                      [-1, 0, 1],\n",
    "                                      [-1, 0, 1]])\n",
    "        vertical_filter = np.array([[1, 1, 1],\n",
    "                                    [0, 0, 0],\n",
    "                                    [-1, -1, -1]])\n",
    "\n",
    "        # Applying horizontal pattern filter\n",
    "        delta_horizontal = cv2.filter2D(src=gray_img.astype(np.float), ddepth=-1,\n",
    "                                        kernel=horizontal_filter)\n",
    "        for wi in range(0, w - 1):\n",
    "            delta_horizontal[0][wi] = gray_img[0][wi + 1] - gray_img[0][wi]\n",
    "            delta_horizontal[h - 1][wi] = gray_img[h - 1][wi + 1] - gray_img[h - 1][wi]\n",
    "        for hi in range(0, h):\n",
    "            delta_horizontal[hi][0] = gray_img[hi][1] - gray_img[hi][0]\n",
    "            delta_horizontal[hi][w - 1] = gray_img[hi][w - 1] - gray_img[hi][w - 2]\n",
    "\n",
    "        # Applying vertical pattern filter\n",
    "        delta_vertical = cv2.filter2D(src=gray_img.astype(np.float), ddepth=-1,\n",
    "                                      kernel=vertical_filter)\n",
    "        for wi in range(0, w):\n",
    "            delta_vertical[0][wi] = gray_img[1][wi] - gray_img[0][wi]\n",
    "            delta_vertical[h - 1][wi] = gray_img[h - 1][wi] - gray_img[h - 2][wi]\n",
    "        for hi in range(0, h - 1):\n",
    "            delta_vertical[hi][0] = gray_img[hi + 1][0] - gray_img[hi][0]\n",
    "            delta_vertical[hi][w - 1] = gray_img[hi + 1][w - 1] - gray_img[hi][w - 1]\n",
    "\n",
    "        delta_magnitude = (np.abs(delta_horizontal) + np.abs(delta_vertical)) / 2.0\n",
    "        delta_magnitude_vec = delta_magnitude.reshape(-1)\n",
    "\n",
    "        # Calculate the angle (theta)\n",
    "        theta = np.zeros([h, w])\n",
    "        for row in range(h):\n",
    "            for col in range(w):\n",
    "                if delta_horizontal[row][col] == 0 and delta_vertical[row][col] == 0:\n",
    "                    theta[row][col] = 0\n",
    "                elif delta_horizontal[row][col] == 0:\n",
    "                    theta[row][col] = np.pi\n",
    "                else:\n",
    "                    theta[row][col] = np.arctan(delta_vertical[row][col] / delta_horizontal[row][col]) + np.pi / 2.0\n",
    "        theta_vec = theta.reshape(-1)\n",
    "\n",
    "        n = 16\n",
    "        thresh = 12\n",
    "        HD = np.zeros(n)\n",
    "\n",
    "        magnitude_len = delta_magnitude_vec.shape[0]\n",
    "        for ni in range(n):\n",
    "            for k in range(magnitude_len):\n",
    "                if (delta_magnitude_vec[k] >= thresh) and \\\n",
    "                   (theta_vec[k] >= (2 * ni - 1) * np.pi / (2 * n)) and \\\n",
    "                   (theta_vec[k] < (2 * ni + 1) * np.pi / (2 * n)):\n",
    "                    HD[ni - 1] += 1\n",
    "        HD = HD / np.sum(HD)\n",
    "\n",
    "        directionality = 0.0\n",
    "        HD_max_index = np.argmax(HD)\n",
    "        for ni in range(n):\n",
    "            directionality += np.power((ni - HD_max_index), 2) * HD[ni]\n",
    "\n",
    "        return directionality\n",
    "\n",
    "    def compute_tamura(self):\n",
    "        coarseness = self.coarseness()\n",
    "        contrast = self.contrast()\n",
    "        directionality = self.directionality()\n",
    "\n",
    "        return [coarseness, contrast, directionality]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_263471/3475560424.py:10: FutureWarning: `selem` is a deprecated argument name for `entropy`. It will be removed in version 1.0. Please use `footprint` instead.\n",
      "  entropy = skimage.filters.rank.entropy(image=self.gray_img,\n",
      "/tmp/ipykernel_263471/3475560424.py:127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  delta_horizontal = cv2.filter2D(src=gray_img.astype(np.float), ddepth=-1,\n",
      "/tmp/ipykernel_263471/3475560424.py:137: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  delta_vertical = cv2.filter2D(src=gray_img.astype(np.float), ddepth=-1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1893659603262525]\n",
      "[10.84144, 108.4337695180842, 18.400310366232155]\n"
     ]
    }
   ],
   "source": [
    "texture = Texture(\"/home/michael_getachew/creative-optimization/creative-optimisation-cv/data/Challenge_Data/Assets/0a22f881b77f00220f2034c21a18b854/_preview.png\")\n",
    "entropy_res = texture.compute_entropy()\n",
    "tamura_res = texture.compute_tamura()\n",
    "print(entropy_res)\n",
    "print(tamura_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of Aesthetic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "class ExtractorPipeline():\n",
    "    \"\"\"\n",
    "    performs feature extraction from all image files in the assets folder\n",
    "    Creates a directory where extracted features will be saved as CSV files\n",
    "    Extracted features include: Logo, CTA button, engagement button, objects, facial features, dominant colours, texts\n",
    "    Parameters: full path to the assets folder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder) -> None:\n",
    "        self.assets_folder = data_folder\n",
    "        assets_dir_path = os.path.dirname(self.assets_folder)\n",
    "        self.extracted_path = str(\n",
    "            Path(assets_dir_path).parent)+\"/Assets\"\n",
    "\n",
    "        if not os.path.isdir(self.extracted_path):\n",
    "            os.makedirs(self.extracted_path)\n",
    "\n",
    "    def aesthetic_extractor(self):\n",
    "        \"\"\"\n",
    "        extract the location of the logo from all preview images in the assets folder\n",
    "        \"\"\"\n",
    "        folder_list = glob.glob(self.assets_folder)\n",
    "\n",
    "        aesthetic_featues = []\n",
    "\n",
    "        for folder in folder_list:\n",
    "            # access folders in the assets directory\n",
    "            query_img = os.path.join(folder, '_preview.png')\n",
    "\n",
    "            # check if files exist\n",
    "            if os.path.exists(query_img):\n",
    "                \n",
    "                #COLOR\n",
    "                image = ImageColor(query_img)\n",
    "                Ad_color_diversity = image.compute_color_diversity()\n",
    "                Ad_color_valence = image.compute_valence()\n",
    "                Ad_color_arousal = image.compute_arousal()\n",
    "                Ad_color_dominace = image.compute_dominance()\n",
    "                \n",
    "                #COMPOSITION\n",
    "                composition = Composition(query_img)\n",
    "                Ad_hue_dof = composition.compute_h_dof()\n",
    "                Ad_saturation_dof = composition.compute_s_dof()\n",
    "                Ad_value_dof = composition.compute_v_dof()\n",
    "                Ad_edge_pixel = composition.compute_edge_pixels()\n",
    "                Ad_s_mean = composition.compute_s_mean()\n",
    "                Ad_v_mean = composition.compute_v_mean()\n",
    "                \n",
    "                #TEXTURE\n",
    "                texture = Texture(query_img)\n",
    "                Ad_entropy = texture.compute_entropy()\n",
    "                Ad_coarseness = texture.coarseness()\n",
    "                Ad_directionality = texture.directionality()\n",
    "                Ad_contrast = texture.contrast()\n",
    "                \n",
    "                aesthetic_featues.append([folder.split('/')[-1], Ad_color_diversity,Ad_color_valence,Ad_color_arousal,Ad_color_dominace,\n",
    "                Ad_hue_dof,Ad_saturation_dof,Ad_value_dof,Ad_edge_pixel,Ad_s_mean,Ad_v_mean,Ad_entropy, Ad_coarseness, Ad_directionality, Ad_contrast])\n",
    "            else:\n",
    "                # if image does not exist\n",
    "                aesthetic_featues.append([folder.split('/')[-1], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "        # save the list elements as a dataframe\n",
    "        df = pd.DataFrame(aesthetic_featues, columns=['Ad_color_diversity','Ad_color_valence','Ad_color_arousal','Ad_color_dominace',\n",
    "                'Ad_hue_dof','Ad_saturation_dof','Ad_value_dof','Ad_edge_pixel','Ad_s_mean','Ad_v_mean','Ad_entropy','Ad_coarseness','Ad_directionality','Ad_contrast'])\n",
    "\n",
    "        # save dataframe as csv file\n",
    "        df.to_csv(self.extracted_path+'/aesthetic_featues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ExtractorPipeline at 0x7f0319bab460>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExtractorPipeline(\"/home/michael_getachew/creative-optimization/creative-optimisation-cv/data/Challenge_Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc129abd2f7491e5923cf05ccd2ab6ea906a3ae76280879defadd05f41862ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
