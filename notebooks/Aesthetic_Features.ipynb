{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aesthetic Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By Aesthetic feature extraction we refer to the aesthetical components of an image this mainly refers to Color, Composition and Texture of an adcreative. This features are extracted on the top of the assumption the peoples are attracted to beauty as a whole eventhough different persons have different perception of beauty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageColor():\n",
    "    def __init__(self, image_path):\n",
    "        super(ImageColor, self).__init__()\n",
    "\n",
    "        # Read in the image\n",
    "        self.bgr_img = None\n",
    "        self.hsv_img = None\n",
    "        self.gray_img = None\n",
    "\n",
    "        self.h_mean, self.s_mean, self.v_mean = None, None, None\n",
    "        self.h_std, self.s_std, self.v_std = None, None, None\n",
    "\n",
    "        # Read in the image\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def update(self, image_path):\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def read_in(self, image_path):\n",
    "        # This function reads the image into the bgr(blue, green, red), hsv(hue, saturation, value) and gray_scale image.\n",
    "\n",
    "        self.bgr_img = cv2.imread(image_path)\n",
    "        self.hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "        self.gray_img = cv2.imread(image_path, 0)\n",
    "\n",
    "        # Compute the statics\n",
    "        self.h_mean, self.s_mean, self.v_mean = np.mean(self.hsv_img, axis=(0, 1))\n",
    "        self.h_std, self.s_std, self.v_std = np.std(self.hsv_img, axis=(0, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_circular(channel_image):\n",
    "        A = np.cos(channel_image).sum()\n",
    "        B = np.sin(channel_image).sum()\n",
    "\n",
    "        R = 1 - np.sqrt(A ** 2 + B ** 2) / (channel_image.shape[0] * channel_image.shape[1])\n",
    "\n",
    "        return R\n",
    "\n",
    "    def compute_hsv_statics(self):\n",
    "        h_circular = self.compute_circular(self.hsv_img[0])\n",
    "        v_intensity = np.sqrt((self.hsv_img[-1] ** 2).mean())\n",
    "\n",
    "        return [self.s_mean, self.s_std, self.v_std, h_circular, v_intensity]\n",
    "\n",
    "    def compute_emotion_based(self):\n",
    "        valence = 0.69 * self.v_mean + 0.22 * self.s_mean\n",
    "        arousal = -0.31 * self.v_mean + 0.6 * self.s_mean\n",
    "        dominance = -0.76 * self.v_mean + 0.32 * self.s_mean\n",
    "\n",
    "        return [valence, arousal, dominance]\n",
    "    \n",
    "    def compute_color_diversity(self):\n",
    "        \"\"\"Adapted from\n",
    "        https://github.com/yilangpeng/computational-aesthetics/blob/27ff52b47b880bd46a14a7b062a4dde69b6a9988/basic.py#L46-L56\n",
    "        \"\"\"\n",
    "        rgb = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2RGB).astype(float)\n",
    "\n",
    "        l_rgbR, l_rgbG, l_rgbB = cv2.split(rgb)\n",
    "        l_rg = l_rgbR - l_rgbG\n",
    "        l_yb = 0.5 * l_rgbR + 0.5 * l_rgbG - l_rgbB\n",
    "\n",
    "        rg_sd = np.std(l_rg)\n",
    "        rg_mean = np.mean(l_rg)\n",
    "        yb_sd = np.std(l_yb)\n",
    "        yb_mean = np.mean(l_yb)\n",
    "\n",
    "        rg_yb_sd = (rg_sd ** 2 + yb_sd ** 2) ** 0.5\n",
    "        rg_yb_mean = (rg_mean ** 2 + yb_mean ** 2) ** 0.5\n",
    "        colorful = rg_yb_sd + (rg_yb_mean * 0.3)\n",
    "\n",
    "        return [colorful]\n",
    "\n",
    "    def compute_color_info(self):\n",
    "        hsv_res = self.compute_hsv_statics()\n",
    "        emotion_res = self.compute_emotion_based()\n",
    "        color_div_res = self.compute_color_diversity()\n",
    "\n",
    "        return hsv_res + emotion_res + color_div_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.311312694085938]\n",
      "[137.71552599999998, -34.069985111111116, -129.31574844444444]\n"
     ]
    }
   ],
   "source": [
    "image = ImageColor(\"/home/michael_getachew/creative-optimization/creative-optimisation-cv/data/Challenge_Data/Assets/0a59be2e7dd53d6de11a10ce3649c081/_preview.png\")\n",
    "res_diversity = image.compute_color_diversity()\n",
    "res_emotion = image.compute_emotion_based()\n",
    "res_hsvstat = image.compute_hsv_statics()\n",
    "print(res_diversity)\n",
    "print(res_emotion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.cluster import (\n",
    "    MeanShift,\n",
    "    estimate_bandwidth\n",
    ")\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Composition():\n",
    "    def __init__(self, image_path):\n",
    "        super(Composition, self).__init__()\n",
    "        self.bgr_img = None\n",
    "        self.hsv_img = None\n",
    "        self.gray_img = None\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def compute_edge_pixels(self,\n",
    "                            blur_size: int = 3,\n",
    "                            ratio_low=0.4, ratio_up=0.8):\n",
    "        \"\"\"Adapted from\n",
    "        https://github.com/yilangpeng/computational-aesthetics/blob/master/edge.py\n",
    "        \"\"\"\n",
    "        h, w = self.bgr_img.shape[:2]\n",
    "        blur_img = cv2.GaussianBlur(self.gray_img, (blur_size, blur_size), 0)\n",
    "\n",
    "        thresh_low = min(100, np.quantile(blur_img, q=ratio_low))\n",
    "        thresh_up = max(200, np.quantile(blur_img, q=ratio_up))\n",
    "\n",
    "        edges_img = cv2.Canny(blur_img,\n",
    "                              threshold1=thresh_low,\n",
    "                              threshold2=thresh_up)\n",
    "        num_edges = np.count_nonzero(edges_img) / (h * w)\n",
    "\n",
    "        return [num_edges]\n",
    "\n",
    "    def update(self, image_path):\n",
    "        self.read_in(image_path)\n",
    "\n",
    "    def read_in(self, image_path):\n",
    "        self.bgr_img = cv2.imread(image_path)\n",
    "        self.hsv_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2HSV)\n",
    "        self.gray_img = cv2.imread(image_path, 0)\n",
    "\n",
    "    def compute_level_of_details(self,\n",
    "                                 quantile=0.2,\n",
    "                                 n_samples=3000,\n",
    "                                 thresh=0.05):\n",
    "        # Using quick shift segmentation\n",
    "        rgb_img = cv2.cvtColor(self.bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Flatten the image\n",
    "        flat_image = rgb_img.reshape(-1, 3)\n",
    "        flat_image = np.float32(flat_image)\n",
    "\n",
    "        bandwidth = estimate_bandwidth(flat_image,\n",
    "                                       quantile=quantile,\n",
    "                                       n_samples=n_samples)\n",
    "        mean_shift = MeanShift(bandwidth, bin_seeding=True)\n",
    "        mean_shift.fit(flat_image)\n",
    "        image_labels = mean_shift.labels_\n",
    "\n",
    "        h, w = rgb_img.shape[:2]\n",
    "        unique_labels, unique_counts = np.unique(image_labels,\n",
    "                                                 return_counts=True)\n",
    "\n",
    "        # Remove small region of image\n",
    "        mean_thresh = (h * w) * thresh\n",
    "        unique_labels = unique_labels[unique_counts > mean_thresh]\n",
    "        unique_counts = unique_counts[unique_counts > mean_thresh]\n",
    "\n",
    "        num_seg = len(unique_labels)\n",
    "        average_size = unique_counts.mean() / (h * w)\n",
    "\n",
    "        return [num_seg, average_size]\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_channel_depth_of_field(channel):\n",
    "        level_wanted = channel[1]\n",
    "        h, w = level_wanted[0].shape[:2]\n",
    "\n",
    "        # Create blank image to include all channel\n",
    "        blank_image = np.zeros((h, w, 3))\n",
    "\n",
    "        for idx, level_wanted_matrix in enumerate(level_wanted):\n",
    "            blank_image[..., idx] = np.abs(level_wanted_matrix)\n",
    "\n",
    "        \n",
    "        # Compute M6, M7, M10, M11\n",
    "        start_x, end_x = int(h / 4), int(3 * h / 4)\n",
    "        start_y, end_y = int(w / 4), int(3 * w / 4)\n",
    "\n",
    "        dof_channel = np.sum(blank_image[start_x:end_x, start_y:end_y, :]) / np.sum(blank_image)\n",
    "\n",
    "        return dof_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc129abd2f7491e5923cf05ccd2ab6ea906a3ae76280879defadd05f41862ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
